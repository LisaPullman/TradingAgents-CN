#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¡…åŸºæµåŠ¨ (SiliconFlow) API é€‚é…å™¨
æ”¯æŒå¤šç§æ¨¡å‹çš„ç»Ÿä¸€æ¥å£ï¼ŒåŒ…æ‹¬ DeepSeekã€Qwenã€Claudeã€GPT ç­‰
"""

import os
from typing import Optional, Dict, Any, List
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage
from langchain_core.outputs import ChatResult, ChatGeneration

# é¿å…å¾ªç¯å¯¼å…¥ï¼Œç›´æ¥ç»§æ‰¿ChatOpenAI
from langchain_openai import ChatOpenAI


class ChatSiliconFlow(ChatOpenAI):
    """ç¡…åŸºæµåŠ¨ OpenAI å…¼å®¹é€‚é…å™¨"""
    
    def __init__(
        self,
        model: str = "deepseek-ai/DeepSeek-V3",
        api_key: Optional[str] = None,
        base_url: str = "https://api.siliconflow.cn/v1",
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–ç¡…åŸºæµåŠ¨é€‚é…å™¨
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸ºdeepseek-chat
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡SILICONFLOW_API_KEYè·å–
            base_url: APIåŸºç¡€URL
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        """
        
        # è·å–APIå¯†é’¥
        if api_key is None:
            api_key = os.getenv("SILICONFLOW_API_KEY")
            if not api_key:
                raise ValueError("ç¡…åŸºæµåŠ¨ APIå¯†é’¥æœªæ‰¾åˆ°ã€‚è¯·è®¾ç½®SILICONFLOW_API_KEYç¯å¢ƒå˜é‡æˆ–ä¼ å…¥api_keyå‚æ•°ã€‚")
        
        # åˆå§‹åŒ–çˆ¶ç±» - ç›´æ¥ä½¿ç”¨ChatOpenAIçš„å‚æ•°
        super().__init__(
            model=model,
            api_key=api_key,
            base_url=base_url,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        
        self.model_name = model
        print(f"âœ… ç¡…åŸºæµåŠ¨é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"   æ¨¡å‹: {model}")
        print(f"   API Base: {base_url}")


# æ”¯æŒçš„ç¡…åŸºæµåŠ¨æ¨¡å‹é…ç½®ï¼ˆåŸºäºå®é™…æ”¯æŒçš„æ¨¡å‹ï¼‰
SILICONFLOW_MODELS = {
    # DeepSeek ç³»åˆ— - ç¡…åŸºæµåŠ¨ä¸»è¦æ”¯æŒçš„æ¨¡å‹
    "deepseek-ai/DeepSeek-V3": {
        "description": "DeepSeek V3 - æœ€æ–°ç‰ˆæœ¬ï¼Œæ¨ç†èƒ½åŠ›å¼º",
        "context_length": 64000,
        "supports_function_calling": True,
        "recommended_for": ["é€šç”¨å¯¹è¯", "ä»£ç åˆ†æ", "é€»è¾‘æ¨ç†", "é‡‘èåˆ†æ"],
        "provider": "deepseek",
        "alias": "deepseek-chat"
    },
    "deepseek-ai/DeepSeek-R1": {
        "description": "DeepSeek R1 - æ¨ç†ä¸“ç”¨æ¨¡å‹",
        "context_length": 64000,
        "supports_function_calling": True,
        "recommended_for": ["å¤æ‚æ¨ç†", "æ•°å­¦è®¡ç®—", "é€»è¾‘åˆ†æ"],
        "provider": "deepseek"
    },
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": {
        "description": "DeepSeek R1 è’¸é¦ç‰ˆ - è½»é‡çº§æ¨ç†æ¨¡å‹",
        "context_length": 32000,
        "supports_function_calling": True,
        "recommended_for": ["å¿«é€Ÿæ¨ç†", "æˆæœ¬ä¼˜åŒ–", "ç®€å•åˆ†æ"],
        "provider": "deepseek"
    },

    # Qwen ç³»åˆ—
    "Qwen/QwQ-32B-Preview": {
        "description": "é€šä¹‰åƒé—® QwQ-32B - é—®ç­”ä¸“ç”¨æ¨¡å‹",
        "context_length": 32000,
        "supports_function_calling": True,
        "recommended_for": ["é—®ç­”ä»»åŠ¡", "çŸ¥è¯†æŸ¥è¯¢", "ä¿¡æ¯æ£€ç´¢"],
        "provider": "alibaba"
    },
    "Qwen/Qwen2.5-72B-Instruct": {
        "description": "é€šä¹‰åƒé—® 2.5-72B - æŒ‡ä»¤è·Ÿéšæ¨¡å‹",
        "context_length": 32000,
        "supports_function_calling": True,
        "recommended_for": ["å¤æ‚æŒ‡ä»¤", "ä¸“ä¸šä»»åŠ¡", "æ·±åº¦åˆ†æ"],
        "provider": "alibaba"
    },
    "Qwen/Qwen2.5-32B-Instruct": {
        "description": "é€šä¹‰åƒé—® 2.5-32B - å¹³è¡¡æ€§èƒ½æ¨¡å‹",
        "context_length": 32000,
        "supports_function_calling": True,
        "recommended_for": ["æ—¥å¸¸ä»»åŠ¡", "ä¸­ç­‰å¤æ‚åº¦åˆ†æ"],
        "provider": "alibaba"
    },
    "Qwen/Qwen2.5-14B-Instruct": {
        "description": "é€šä¹‰åƒé—® 2.5-14B - è½»é‡çº§æ¨¡å‹",
        "context_length": 32000,
        "supports_function_calling": True,
        "recommended_for": ["å¿«é€Ÿä»»åŠ¡", "æˆæœ¬ä¼˜åŒ–"],
        "provider": "alibaba"
    },

    # GLM ç³»åˆ—
    "THUDM/GLM-4-9B-Chat": {
        "description": "GLM-4-9B - æ¸…åå¤§å­¦å¼€æºæ¨¡å‹",
        "context_length": 32000,
        "supports_function_calling": True,
        "recommended_for": ["ä¸­æ–‡å¯¹è¯", "å­¦æœ¯åˆ†æ"],
        "provider": "thudm"
    },

    # Meta Llama ç³»åˆ—
    "meta-llama/Llama-3.1-70B-Instruct": {
        "description": "Llama 3.1 70B - Metaå¼€æºå¤§æ¨¡å‹",
        "context_length": 128000,
        "supports_function_calling": True,
        "recommended_for": ["é•¿æ–‡æœ¬å¤„ç†", "å¤æ‚æ¨ç†"],
        "provider": "meta"
    },
    "meta-llama/Llama-3.1-8B-Instruct": {
        "description": "Llama 3.1 8B - è½»é‡çº§ç‰ˆæœ¬",
        "context_length": 128000,
        "supports_function_calling": True,
        "recommended_for": ["å¿«é€Ÿä»»åŠ¡", "èµ„æºå—é™ç¯å¢ƒ"],
        "provider": "meta"
    }
}


def get_available_siliconflow_models() -> Dict[str, Dict[str, Any]]:
    """è·å–å¯ç”¨çš„ç¡…åŸºæµåŠ¨æ¨¡å‹åˆ—è¡¨"""
    return SILICONFLOW_MODELS


def create_siliconflow_llm(
    model: str = "deepseek-ai/DeepSeek-V3",
    api_key: Optional[str] = None,
    temperature: float = 0.1,
    max_tokens: int = 2000,
    **kwargs
) -> ChatSiliconFlow:
    """åˆ›å»ºç¡…åŸºæµåŠ¨ LLM å®ä¾‹çš„ä¾¿æ·å‡½æ•°"""
    
    return ChatSiliconFlow(
        model=model,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs
    )


def get_model_recommendations(task_type: str = "general") -> List[str]:
    """
    æ ¹æ®ä»»åŠ¡ç±»å‹æ¨èæœ€é€‚åˆçš„æ¨¡å‹ï¼ˆåŸºäºç¡…åŸºæµåŠ¨å®é™…æ”¯æŒçš„æ¨¡å‹ï¼‰

    Args:
        task_type: ä»»åŠ¡ç±»å‹ ("general", "coding", "analysis", "fast", "cost_effective", "financial", "chinese", "reasoning")

    Returns:
        æ¨èçš„æ¨¡å‹åˆ—è¡¨
    """
    recommendations = {
        "general": ["deepseek-ai/DeepSeek-V3", "Qwen/Qwen2.5-32B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"],
        "coding": ["deepseek-ai/DeepSeek-V3", "deepseek-ai/DeepSeek-R1", "Qwen/Qwen2.5-72B-Instruct"],
        "analysis": ["deepseek-ai/DeepSeek-V3", "Qwen/Qwen2.5-72B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"],
        "fast": ["deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", "Qwen/Qwen2.5-14B-Instruct", "meta-llama/Llama-3.1-8B-Instruct"],
        "cost_effective": ["deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", "Qwen/Qwen2.5-14B-Instruct", "THUDM/GLM-4-9B-Chat"],
        "financial": ["deepseek-ai/DeepSeek-V3", "Qwen/Qwen2.5-72B-Instruct", "deepseek-ai/DeepSeek-R1"],
        "chinese": ["Qwen/Qwen2.5-72B-Instruct", "Qwen/Qwen2.5-32B-Instruct", "THUDM/GLM-4-9B-Chat"],
        "reasoning": ["deepseek-ai/DeepSeek-R1", "deepseek-ai/DeepSeek-V3", "Qwen/QwQ-32B-Preview"],
        "qa": ["Qwen/QwQ-32B-Preview", "deepseek-ai/DeepSeek-V3", "Qwen/Qwen2.5-72B-Instruct"]
    }

    return recommendations.get(task_type, recommendations["general"])


def test_siliconflow_connection(api_key: Optional[str] = None) -> bool:
    """
    æµ‹è¯•ç¡…åŸºæµåŠ¨è¿æ¥
    
    Args:
        api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è·å–
    
    Returns:
        è¿æ¥æ˜¯å¦æˆåŠŸ
    """
    try:
        llm = create_siliconflow_llm(
            model="deepseek-ai/DeepSeek-V3",
            api_key=api_key,
            max_tokens=50
        )
        
        # å‘é€æµ‹è¯•æ¶ˆæ¯
        response = llm.invoke([HumanMessage(content="ä½ å¥½ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'")])
        
        if response and response.content:
            print("âœ… ç¡…åŸºæµåŠ¨è¿æ¥æµ‹è¯•æˆåŠŸ")
            return True
        else:
            print("âŒ ç¡…åŸºæµåŠ¨è¿æ¥æµ‹è¯•å¤±è´¥ï¼šæ— å“åº”")
            return False
            
    except Exception as e:
        print(f"âŒ ç¡…åŸºæµåŠ¨è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    # æµ‹è¯•ç¡…åŸºæµåŠ¨é€‚é…å™¨
    print("ğŸ§ª ç¡…åŸºæµåŠ¨é€‚é…å™¨æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥APIå¯†é’¥
    api_key = os.getenv('SILICONFLOW_API_KEY')
    if not api_key:
        print("âŒ æœªæ‰¾åˆ° SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡")
        print("ğŸ’¡ è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export SILICONFLOW_API_KEY=your_api_key")
        exit(1)
    
    print(f"âœ… APIå¯†é’¥: {api_key[:10]}...")
    
    # æµ‹è¯•è¿æ¥
    if test_siliconflow_connection():
        print("\nğŸ‰ ç¡…åŸºæµåŠ¨é€‚é…å™¨æµ‹è¯•é€šè¿‡ï¼")
        
        # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
        print("\nğŸ“‹ å¯ç”¨æ¨¡å‹:")
        models = get_available_siliconflow_models()
        for model_name, info in models.items():
            print(f"  â€¢ {model_name}: {info['description']}")
        
        # æ˜¾ç¤ºæ¨èæ¨¡å‹
        print("\nğŸ’¡ é‡‘èåˆ†ææ¨èæ¨¡å‹:")
        financial_models = get_model_recommendations("financial")
        for model in financial_models:
            print(f"  â€¢ {model}")
    else:
        print("\nâŒ ç¡…åŸºæµåŠ¨é€‚é…å™¨æµ‹è¯•å¤±è´¥")
